head(y_pred)
summary(mod)
plot(mod,which=1)
fit1 <- glm(chd~., data=SAheart, family=binomial)
fit0 <- glm(chd~1, data=SAheart, family=binomial)
library(MASS)
step <-stepAIC(fit0,direction="both",scope=list(upper=fit1,lower=fit0))
mod <- glm(chd ~ age + famhist + tobacco + typea + ldl, data=SAheart, family=binomial)
mod$coefficients
step$residuals
plot(predict(step),residuals(step))
coefficients(step)
# predecimos 1 si la probabilidad es mayor que 0.5 y 0 si es menor que 0.5 y comparamos
y_pred <- predict(step)>.5
head(y_pred)
head(SAheart$chd)
y_pred <- as.numeric(predict(mod, subset(SAheart,select = -chd ), type="response")>.5)
y <- as.numeric(SAheart$chd)
install.packages(c("e1071", "caret", "e1071")
# install.packages(c("e1071", "caret", "e1071")
library(caret)
install.packages(c("e1071", "caret", "e1071"))
library(caret)
library(ggplot2)
library(lattice)
library(e1071)
confusionMatrix(y, y_pred, mode="everything")
y_pred <- as.numeric(predict(mod, subset(SAheart,select = -chd ), type="response")>.5)
y <- as.numeric(SAheart$chd)
install.packages(c("e1071", "caret", "e1071"))
library(caret)
library(ggplot2)
library(lattice)
library(e1071)
confusionMatrix(y, y_pred, mode="everything")
install.packages(c("e1071", "caret", "e1071"))
y_pred <- as.numeric(predict(mod, subset(SAheart,select = -chd ), type="response")>.5)
y <- as.numeric(SAheart$chd)
# install.packages(c("e1071", "caret", "e1071"))
library(caret)
library(ggplot2)
library(lattice)
library(e1071)
confusionMatrix(y, y_pred, mode="everything")
coef(mod)
exp(coef(mod))
exp(confint(mod))
library(MASS)
data(Boston)
colnames(Boston)
str(Boston)
# X <- as.matrix(subset(Boston, select= - medv))
# y <- as.matrix(Boston$medv)
X <- data.matrix(subset(Boston, select= - medv))
y <- Boston$medv
str(X)
install.packages("glmnet")
# install.packages("glmnet")
library(glmnet)
cv.ridge <- cv.glmnet(X, y, family='gaussian', alpha=0, parallel=TRUE, standardize=TRUE, type.measure='mse')
# install.packages("glmnet")
library(glmnet)
set.seed(999)
cv.ridge <- cv.glmnet(X, y, family='gaussian', alpha=0, parallel=TRUE, standardize=TRUE, type.measure='mse')
# Resultados
plot(cv.ridge)
#este es el mejor valor de lambda
cv.ridge$lambda.min
#este es el valor del error que se estima para ese valor lambda mínimo dado en MSE
min(cv.ridge$cvm)
#este es el mejor valor de lambda
cv.ridge$lambda.min
#este es el valor del error que se estima para ese valor lambda mínimo dado en MSE
min(cv.ridge$cvm)
var(y)
coef(cv.ridge, s=cv.ridge$lambda.min)
predict.glmnet(cv.ridge$glmnet.fit, newx=X[1:2,], s=cv.ridge$lambda.min)
cv.lasso <- cv.glmnet(X, y, family='gaussian', alpha=1, parallel=TRUE, standardize=TRUE, type.measure='mse')
# Resultados
plot(cv.lasso)
#este es el mejor valor de lambda
cv.lasso$lambda.min
#este es el valor del error que se estima para ese valor lambda mínimo dado en MSE
min(cv.lasso$cvm)
coef(cv.lasso, s=cv.lasso$lambda.min)
predict.glmnet(cv.lasso$glmnet.fit, newx=X[1:2,], s=cv.lasso$lambda.min)
install.packages("ElemStatLearn")
# install.packages("ElemStatLearn")
library(ElemStatLearn)
data("SAheart")
help(SAheart)
head(SAheart)
#la variable categórica a relacionar es chd
str(SAheart)
X <- data.matrix(subset(SAheart  , select= - chd))
y <- as.double(as.matrix(SAheart$chd  ))
str(X)
dim(X)
X_train <- X[1:362,]
y_train <- y[1:362]
X_test <- X[363:462,]
y_test <- y[363:462]
set.seed(999)
cv.ridge <- cv.glmnet(X_train, y_train, family='binomial', alpha=0, parallel=TRUE, standardize=TRUE, type.measure='auc')
# Resultados
plot(cv.ridge)
#este es el mejor valor de lambda
cv.ridge$lambda.min
#este es el valor del error que se estima para ese valor lambda mínimo dado en MSE
max(cv.ridge$cvm)
coef(cv.ridge, s=cv.ridge$lambda.min)
y_pred <- as.numeric(predict.glmnet(cv.ridge$glmnet.fit, newx=X_test, s=cv.ridge$lambda.min)>.5)
#install.packages(c("e1071", "caret", "e1071")
library(caret)
library(ggplot2)
library(lattice)
library(e1071)
confusionMatrix(y_test, y_pred, mode="everything")
# install.packages("glmnet")
library(glmnet)
set.seed(999)
cv.lasso <- cv.glmnet(X_train, y_train, family='binomial', alpha=1, parallel=TRUE, standardize=TRUE, type.measure='auc')
# Resultados
plot(cv.lasso)
#este es el mejor valor de lambda
cv.lasso$lambda.min
#este es el valor del error que se estima para ese valor lambda mínimo dado en MSE
max(cv.lasso$cvm)
coef(cv.lasso, s=cv.lasso$lambda.min)
y_pred <- as.numeric(predict.glmnet(cv.lasso$glmnet.fit, newx=X_test, s=cv.lasso$lambda.min)>.5)
confusionMatrix(y_test, y_pred, mode="everything")
url <- "https://archive.ics.uci.edu/ml/machine-learning-databases/breast-cancer-wisconsin/wdbc.data"
df <- read.csv(url, header = FALSE)
df <- read.csv(url, header = FALSE)
write.csv(df, 'wisconsin_breast.csv')
dim(df)
head(df)
str(df)
X <- df[,3:32]
dim(X)
y <- as.numeric(df$V2) -1
unique(y)
# y[y=='M'] <- 1
# y[y=='B'] <- 0
head(y,20)
head(df$V2, 20)
X_train <- X[1:400,]
y_train <- y[1:400]
X_test <- X[401:569,]
y_test <- y[401:569]
fit1 <- glm(V2~., data=df_train, family=binomial)
url <- "https://archive.ics.uci.edu/ml/machine-learning-databases/breast-cancer-wisconsin/wdbc.data"
df <- read.csv(url, header = FALSE)
df <- read.csv(url, header = FALSE)
write.csv(df, 'wisconsin_breast.csv')
dim(df)
dim(df)
head(df)
str(df)
df$V2 <- as.numeric(df$V2) - 1
df_train <- df[1:400, 1:32]
df_test <- df[401:569, 1:32]
X <- df[,3:32]
dim(X)
y <- df$V2
unique(y)
X_train <- X[1:400,]
y_train <- y[1:400]
X_test <- X[401:569,]
y_test <- y[401:569]
fit1 <- glm(V2~., data=df_train, family=binomial)
fit0 <- glm(V2~1, data=df_train, family=binomial)
library(MASS)
step <-stepAIC(fit0,direction="forward",scope=list(upper=fit1,lower=fit0))
step <-stepAIC(fit0,direction="forward",scope=list(upper=fit1,lower=fit0))
summary(step)
y_pred <- as.numeric(predict(step, X_test, type="response")>.5)
# install.packages(c("e1071", "caret", "e1071")
library(caret)
library(ggplot2)
library(lattice)
library(e1071)
confusionMatrix(y_test, y_pred, mode="everything")
X_train <- data.matrix(X_train)
X_test <- data.matrix(X_test)
# install.packages("glmnet")
library(glmnet)
set.seed(999)
cv.ridge <- cv.glmnet(X_train, y_train, family='binomial', alpha=0, parallel=TRUE, standardize=TRUE, type.measure='auc')
# Resultados
plot(cv.ridge)
#este es el mejor valor de lambda
cv.ridge$lambda.min
#este es el valor del error que se estima para ese valor lambda mínimo dado en MSE
max(cv.ridge$cvm)
coef(cv.ridge, s=cv.ridge$lambda.min)
y_pred <- as.numeric(predict.glmnet(cv.ridge$glmnet.fit, newx=X_test, s=cv.ridge$lambda.min)>.5)
#install.packages(c("e1071", "caret", "e1071")
library(caret)
library(ggplot2)
library(lattice)
library(e1071)
confusionMatrix(y_test, y_pred, mode="everything")
# install.packages("glmnet")
library(glmnet)
set.seed(999)
cv.lasso <- cv.glmnet(X_train, y_train, family='binomial', alpha=1, parallel=TRUE, standardize=TRUE, type.measure='auc')
cv.lasso <- cv.glmnet(X_train, y_train, family='binomial', alpha=1, parallel=TRUE, standardize=TRUE, type.measure='auc')
# Resultados
plot(cv.lasso)
#este es el mejor valor de lambda
cv.lasso$lambda.min
#este es el valor del error que se estima para ese valor lambda mínimo dado en MSE
max(cv.lasso$cvm)
coef(cv.lasso, s=cv.lasso$lambda.min)
y_pred <- as.numeric(predict.glmnet(cv.lasso$glmnet.fit, newx=X_test, s=cv.lasso$lambda.min)>.5)
#install.packages(c("e1071", "caret", "e1071")
library(caret)
library(ggplot2)
library(lattice)
library(e1071)
confusionMatrix(y_test, y_pred, mode="everything")
y_pred <- as.numeric(predict.glmnet(cv.lasso$glmnet.fit, newx=X_test, s=cv.lasso$lambda.min)>.5)
#install.packages(c("e1071", "caret", "e1071")
library(caret)
library(ggplot2)
library(lattice)
library(e1071)
confusionMatrix(y_test, y_pred, mode="everything")
url <- 'https://newonlinecourses.science.psu.edu/stat504/sites/onlinecourses.science.psu.edu.stat504/files/lesson07/crab/index.txt'
df <- read.csv(url,header= F,sep = '\t')
df <- read.csv(url,header= F,sep = '\t')
write.csv(df, 'crabs.csv')
crab <- read.table(url)
colnames(crab) <- c("Obs","C","S","W","Wt","Sa")
head(crab)
summary(crab)
crab <- crab[,-1]
dim(crab)
crab$C <- as.factor(crab$C)
crab$S <- as.factor(crab$S)
head(crab)
library(MASS)
model <- glm.nb(Sa~1, data = crab)
summary(model)
model$fitted
model$coefficients
model <- glm.nb(Sa~W, data = crab)
summary(model)
library(MASS)
model <- glm.nb(Sa~1, data = crab)
summary(model)
model$fitted
model$coefficients
model <- glm.nb(Sa~W, data = crab)
summary(model)
head(data.frame(crab,pred=model$fitted),20)
plot(crab$W,crab$Sa)
newdt <- data.frame(W=26.3)
predict(model, newdt,type="response")
fit1 <- glm.nb(Sa~., data=crab)
fit0 <- glm.nb(Sa~1, data=crab)
library(MASS)
step <-stepAIC(fit0,direction="forward",scope=list(upper=fit1,lower=fit0))
step
summary(step)
mod <- glm.nb(formula = Sa ~ Wt + C, data = crab, init.theta = 0.9556206187,
link = log)
exp(mod$coefficients)
data.frame(crab,pred=mod$fitted)
confint(mod)
install.packages("faraway")
# install.packages("faraway")
library(faraway)
data(wafer)
help(wafer)
head(wafer)
#observamos que toma valores positivos la variable objetivo
plot(density(wafer$resist))
summary(wafer)
res.lm.logY <- lm(log(resist) ~ x1 + x2 + x3 + x4, data = wafer)
summary(res.lm.logY)
res.glm.Gamma.log <- glm(formula = resist ~ x1 + x2 + x3 + x4,
family  = Gamma(link = "log"),
data    = wafer)
summary(res.glm.Gamma.log)
hist(res.lm.logY$residuals)
hist(res.glm.Gamma.log$residuals)
res.glm.Gamma.identity <- glm(formula = resist ~ x1 + x2 + x3 + x4,
family  = Gamma(link = "identity"),
data    = wafer)
summary(res.glm.Gamma.identity)
hist(res.glm.Gamma.identity$residuals)
res.glm.Gamma.inverse <- glm(formula = resist ~ x1 + x2 + x3 + x4,
family  = Gamma(link = "inverse"),
data    = wafer)
summary(res.glm.Gamma.inverse)
hist(res.glm.Gamma.inverse$residuals)
fit1 <- glm(resist~., data=wafer, family=Gamma(link = "inverse"))
fit0 <- glm(resist~1, data=wafer, family=Gamma(link = "inverse"))
library(MASS)
step <-stepAIC(fit0,direction="forward",scope=list(upper=fit1,lower=fit0))
step$coefficients
summary(step)
install.packages(c("glmnet","ISLR"))
install.packages(c("glmnet", "ISLR"))
# install.packages(c("glmnet","ISLR"))
library(glmnet,help=T)
help("glmnet")
library(glmnet)
library("foreign")
data(iris)
dim(iris)
head(iris)
str(iris)
iris$Species <- as.numeric(iris$Species)
unique(iris$Species)
X <- as.matrix(subset(iris, select=- Species ))
y <- iris$Species
head(X)
summary(mod)
mod <- glmnet(X, y, alpha=0, family = "multinomial",type.multinomial="ungrouped")
summary(mod)
mod <- glmnet(X, y, alpha=0, family = "multinomial",type.multinomial="ungrouped")
summary(mod)
print(mod)
plot(mod,xvar = "lambda", label = TRUE)
coef(mod,s=0)
coef(mod,s=0.1)
coef(mod,s=0.13)
coef(mod,s=0.29)
predict(mod,newx = X[1:2,],s=0,type = "response")
summary(mod)
cvfit <- cv.glmnet(X, y, alpha=0, family = "multinomial",type.multinomial="ungrouped")
plot(cvfit)
coef(cvfit, s = "lambda.min")
data("AirPassengers")
AP <- AirPassengers
frequency(AP)
plot(AP, ylab="Passengers (1000s)", type="o", pch =20)
AP_train <- window(AP, end=c(1959,12))
AP_test <- window(AP, start=1960)
AP_train
AP_test
AP.decompM <- decompose(AP_train, type = "multiplicative")
plot(AP.decompM)
AP.decompM$seasonal
AP.decompM$trend
AP.decompM$random
install.packages("forecast")
# install.packages("forecast")
library(forecast)
ndiffs(log(AP_train))
diff(AP_train)
AP_train
plot(diff(log(AP_train)), ylab="Passengers (1000s)", type="o", pch =20)
t <- seq(1, 144-12, 1)
modelTrend <- lm(formula = AP.decompM$trend ~ t)
summary(modelTrend)
predT <- predict.lm(modelTrend, newdata = data.frame(t))
plot(AP.decompM$trend[7:132] ~ t[7:132], ylab="T(t)", xlab="t",
type="p", pch=20, main = "Componente de tendencia: modelo lineal vs observado")
lines(predT, col="red")
plot(AP.decompM$trend[7:132] ~ t[7:132], ylab="T(t)", xlab="t",
type="p", pch=20, main = "Componente de tendencia: modelo lineal vs observado")
lines(predT, col="red")
Data1960 <- data.frame("T" = 2.58064*seq(123, 134, 1) + 88.84686, S=rep(0,12), e=rep(0,12),
row.names = c("Jan", "Feb", "Mar", "Apr", "May", "Jun", "Jul", "Aug", "Sep", "Oct", "Nov", "Dec"))
Data1960
Data1960$S <- unique(AP.decompM$seasonal)
Data1960
plot(density(AP.decompM$random[7:122]),
main="Error aproximaci?n")
sd_error <- sd(AP.decompM$random[7:122])
sd_error
Data1960$e <- 1
Data1960
#Centro estimaci?n
Data1960$R <- Data1960$T * Data1960$S * Data1960$e
#Extremo sup. 95% confianza
Data1960$O <- Data1960$T * Data1960$S * (Data1960$e+1.95*sd_error)
#Extremo inf. 95% confianza
Data1960$P <- Data1960$T * Data1960$S * (Data1960$e-1.95*sd_error)
Data1960$Real <- AP_test
Data1960
xr = c(123,134)
plot(Data1960$Real, xlim=xr, ylab = "Passengers (100s)", xlab = "Month" , lwd=10)
lines(Data1960$Real, x=seq(123,134,1), lwd=10)
plot(Data1960$Real, xlim=xr, ylab = "Passengers (100s)", xlab = "Month" , lwd=10)
lines(Data1960$Real, x=seq(123,134,1), lwd=10)
lines(Data1960$R, x=seq(123,134,1), col="blue")
lines(Data1960$O, x=seq(123,134,1), col="green")
lines(Data1960$P, x=seq(123,134,1), col="red")
install.packages("TSPred")
# install.packages("TSPred")
library(TSPred)
MAPE(Data1960$Real, Data1960$R)
error <- Data1960$Real / Data1960$R
sd(error)
plot(density(error),
main="Error aproximaci?n")
data("AirPassengers")
AP <- AirPassengers
AP_train <- window(AP, end=c(1959,12))
AP_test <- window(AP, start=1960)
library(forecast)
plot(AP_train)
#nos dice que necesitamos 1 ndiffs
ndiffs(AP_train)
#en los autocorrelogramas de la variable plana vemos que es necesario derivar
acf(AP_train,lag.max=48)
pacf(AP_train,lag.max=48)
tsdisplay(AP_train)
ndiffs(AP_train)
library(tseries)
install.packages("tseries")
library(tseries)
library(tseries)
Air_ts_d1=diff(AP_train, lag=1,differences=1)
plot(Air_ts_d1)
acf(Air_ts_d1,lag.max=48)
pacf(Air_ts_d1,lag.max=48)
tsdisplay(Air_ts_d1)
adf.test(Air_ts_d1)
Air_ts_d1_d12=diff(diff(AP_train, lag=1,differences=1),lag=12,differences=1)
plot(Air_ts_d1_d12)
acf(Air_ts_d1_d12,lag.max=48)
pacf(Air_ts_d1_d12,lag.max=48)
tsdisplay(Air_ts_d1_d12)
adf.test(Air_ts_d1_d12)
library(forecast)
frequency(AP_train)
ndiffs(AP_train)
#como el resultado de ndiffs es 1, hacemos nsdiffs sobre la derivada inmediata 1
nsdiffs(diff(AP_train))
summary(Air_model)
Air_model <- Arima(AP_train,order=c(1,1,1),  seasonal=list(order = c(1, 1, 1)))
summary(Air_model)
library(forecast)
plot(forecast(Air_model,h=24))
tsdisplay(residuals(Air_model))
#la hip?tesis nula es incorrelaci?n de los residuos, que es lo que deseamos
Box.test(residuals(Air_model),type="Ljung",lag=20)
#calculamos la predicci?n a 12 meses (1960), esto calcula intervalso de confianza
pred <- forecast(Air_model,h=12)
pred$mean
library(forecast)
plot(forecast(Air_model,h=24))
tsdisplay(residuals(Air_model))
#la hip?tesis nula es incorrelaci?n de los residuos, que es lo que deseamos
Box.test(residuals(Air_model),type="Ljung",lag=20)
#calculamos la predicci?n a 12 meses (1960), esto calcula intervalso de confianza
pred <- forecast(Air_model,h=12)
pred$mean
pred
#damos el MAPE
library(TSPred)
MAPE(AP_test, pred$mean)
library(forecast)
Auto_Air_model <- auto.arima(AP_train,trace=TRUE, stepwise = FALSE, parallel=TRUE, num.cores = 6 )
plot(forecast(Auto_Air_model,h=24))
summary(Auto_Air_model)
tsdisplay(residuals(Auto_Air_model))
library(forecast)
plot(forecast(Auto_Air_model,h=24))
tsdisplay(residuals(Auto_Air_model))
#la hip?tesis nula es incorrelaci?n de los residuos, que es lo que deseamos
Box.test(residuals(Auto_Air_model),type="Ljung",lag=20)
#calculamos la predicci?n a 12 meses (1960), esto calcula intervalso de confianza
pred <- forecast(Auto_Air_model,h=12)
pred$mean
pred
#damos el MAPE
library(TSPred)
MAPE(AP_test, pred$mean)
df <- read.csv("ACTIVIDAD_6_UD6_absentismo.csv")
head(df)
str(df)
summary(df)
attach(df)
df <- read.csv("ACTIVIDAD_6_UD6_absentismo.csv")
head(df)
str(df)
summary(df)
attach(df)
df$genero <- factor(genero, levels=1:2,labels = c("fem","masc"))
df$tipoescuela <- factor(tipoescuela,levels = 1:2, labels = c("urbana","rural"))
df$programa <- factor(programa, levels=0:3,labels = c("A","B","C","D"))
head(df,30)
tail(df)
str(df)
summary(df)
head(df,30)
tail(df)
str(df)
summary(df)
tapply(df$ausencias,df$genero,mean)
tapply(df$ausencias,df$tipoescuela,mean)
tapply(df$ausencias,df$programa,mean)
oneway.test(df$ausencias ~ df$genero)
oneway.test(df$ausencias ~ df$tipoescuela)
oneway.test(df$ausencias ~ df$programa)
library(MASS)
mod <- model <- glm.nb(ausencias~., data = df)
summary(mod)
fit1 <- glm.nb(ausencias~., data=df)
fit0 <- glm.nb(ausencias~1, data=df)
library(MASS)
step <-stepAIC(fit0,direction="forward",scope=list(upper=fit1,lower=fit0))
step$coefficients
summary(step)
exp(cbind(coef(step),confint(step)))
